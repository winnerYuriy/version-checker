from urllib.parse import urlparse
import requests
import re
from bs4 import BeautifulSoup
from datetime import datetime
import time

class VersionParser:
    def __init__(self):
        self.session = requests.Session()
        # –î–æ–¥–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏, —â–æ–± —Å–∞–π—Ç–∏ –¥—É–º–∞–ª–∏, —â–æ —Ü–µ –±—Ä–∞—É–∑–µ—Ä
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def get_version_from_website(self, url, selector=None):
        """–û—Ç—Ä–∏–º–∞—Ç–∏ –≤–µ—Ä—Å—ñ—é –∑ –≤–µ–±-—Å–∞–π—Ç—É"""
        try:
            print(f"–ü–µ—Ä–µ–≤—ñ—Ä—è—é {url}...")
            
            # –ó–∞—Ç—Ä–∏–º–∫–∞, —â–æ–± –Ω–µ –ø–µ—Ä–µ–≤–∞–Ω—Ç–∞–∂—É–≤–∞—Ç–∏ —Å–∞–π—Ç–∏
            time.sleep(2)
            
            response = self.session.get(url, timeout=30)
            response.raise_for_status()  # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –ø–æ–º–∏–ª–∫–∏ HTTP
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –Ø–∫—â–æ –∑–∞–¥–∞–Ω–æ CSS —Å–µ–ª–µ–∫—Ç–æ—Ä
            if selector and selector.strip():
                element = soup.select_one(selector)
                if element:
                    text = element.get_text(strip=True)
                    version = self.extract_version_from_text(text)
                    if version:
                        return version
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –ø–æ—à—É–∫ –≤–µ—Ä—Å—ñ—ó –≤ —Ç–µ–∫—Å—Ç—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏
            # –ü–æ—à—É–∫ –ø–∞—Ç—Ç–µ—Ä–Ω—ñ–≤ –≤–µ—Ä—Å—ñ–π: v1.2.3, version 2.0, 3.1.4, etc.
            patterns = [
                r'\b(\d+(?:\.\d+){1,3})\b',  # 1.2, 1.2.3, 1.2.3.4, 1.0.7.79
                r'v?(\d+\.\d+\.\d+)',  # 1.2.3
                r'v?(\d+\.\d+)',       # 1.2
                r'Version\s*[:]?\s*(\d+\.\d+\.\d+)',
                r'–í–µ—Ä—Å—ñ—è\s*[:]?\s*(\d+\.\d+\.\d+)',
                r'(\d{4}\.\d+\.\d+)',  # 2023.1.0
            ]
            
            page_text = soup.get_text()
            for pattern in patterns:
                matches = re.findall(pattern, page_text, re.IGNORECASE)
                if matches:
                    # –ë–µ—Ä–µ–º–æ –ø–µ—Ä—à–µ –∑–Ω–∞–π–¥–µ–Ω–µ —á–∏—Å–ª–æ —è–∫ –Ω–∞–π—ñ–º–æ–≤—ñ—Ä–Ω—ñ—à—É –≤–µ—Ä—Å—ñ—é
                    for match in matches:
                        if len(match) > 2:  # –ú—ñ–Ω—ñ–º—É–º 1.2
                            return match
            
            return None
            
        except requests.RequestException as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ {url}: {e}")
            return None
        except Exception as e:
            print(f"–ù–µ–≤—ñ–¥–æ–º–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
            return None
    
    def get_grandstream_version(self, model_name):
        """
        –°–ø–µ—Ü—ñ–∞–ª—å–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –≤–µ—Ä—Å—ñ—ó –ø—Ä–æ—à–∏–≤–∫–∏ –∑ —Å–∞–π—Ç—É Grandstream
        –∑–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—é –º–æ–¥–µ–ª–ª—é –ø—Ä–∏—Å—Ç—Ä–æ—é.
        
        –ü—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:
            get_grandstream_version("GXP1625")
            get_grandstream_version("GXW4232")
        """
        url = "https://www.grandstream.com/support/firmware"
        print(f"üîç –û—Ç—Ä–∏–º—É—é –¥–∞–Ω—ñ –¥–ª—è –º–æ–¥–µ–ª—ñ {model_name} –∑ {url}")
        
        try:
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Å—Ç–æ—Ä—ñ–Ω–∫—É
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –í–°–Ü —Ç–∞–±–ª–∏—Ü—ñ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ
            tables = soup.find_all('table')
            
            # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –Ω–∞–∑–≤—É –º–æ–¥–µ–ª—ñ –¥–ª—è –ø–æ—à—É–∫—É
            search_model = model_name.upper().replace(" ", "")
            
            for table in tables:
                # –®—É–∫–∞—î–º–æ —Ä—è–¥–∫–∏ —Ç–∞–±–ª–∏—Ü—ñ
                rows = table.find_all('tr')
                
                for row in rows:
                    # –ü–µ—Ä—à–∞ –∫–æ–º—ñ—Ä–∫–∞ –≤ —Ä—è–¥–∫—É –∑–∞–∑–≤–∏—á–∞–π –º—ñ—Å—Ç–∏—Ç—å –Ω–∞–∑–≤–∏ –º–æ–¥–µ–ª–µ–π
                    first_cell = row.find('td')
                    if not first_cell:
                        continue
                    
                    # –û—Ç—Ä–∏–º—É—î–º–æ —Ç–µ–∫—Å—Ç –∫–æ–º—ñ—Ä–∫–∏ –∑ –º–æ–¥–µ–ª—è–º–∏
                    models_text = first_cell.get_text(strip=True).upper()
                    
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –Ω–∞—à –ø—Ä–∏—Å—Ç—Ä—ñ–π –≤–∫–∞–∑–∞–Ω–∏–π —É —Ü—å–æ–º—É —Å–ø–∏—Å–∫—É
                    # –í—Ä–∞—Ö–æ–≤—É—î–º–æ —Ñ–æ—Ä–º–∞—Ç–∏ –∑–∞–ø–∏—Å—É: "GXP1610/1615", "GXP1620/1625"
                    # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–ø–∏—Å–æ–∫ –º–æ–∂–ª–∏–≤–∏—Ö –Ω–∞–∑–≤ –¥–ª—è –ø–æ—à—É–∫—É
                    possible_names = [search_model]
                    
                    # –Ø–∫—â–æ —à—É–∫–∞—î–º–æ GXP1625, –¥–æ–¥–∞–º–æ —Ç–∞–∫–æ–∂ GXP1620/1625
                    if "/" not in search_model:
                        # –î–æ–¥–∞—î–º–æ –≤–∞—Ä—ñ–∞–Ω—Ç —ñ–∑ —Å–ª–µ—à–µ–º (–¥–ª—è –ø–æ—à—É–∫—É –≤ —Ä—è–¥–∫–∞—Ö —Ç–∏–ø—É "GXP1620/1625")
                        base_model = re.match(r'([A-Z]+)\d+', search_model)
                        if base_model:
                            base = base_model.group(1)  # "GXP"
                            possible_names.append(base)
                    
                    # –§–ª–∞–≥ –∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
                    model_found = False
                    for name in possible_names:
                        if name and name in models_text:
                            model_found = True
                            break
                    
                    if model_found:
                        # –ó–Ω–∞–π—à–ª–∏ —Ä—è–¥–æ–∫ –∑ –Ω–∞—à–æ—é –º–æ–¥–µ–ª–ª—é
                        # –¢–µ–ø–µ—Ä —à—É–∫–∞—î–º–æ –≤–µ—Ä—Å—ñ—é –ø—Ä–æ—à–∏–≤–∫–∏ –≤ —Ü—å–æ–º—É —Ä—è–¥–∫—É
                        all_cells = row.find_all('td')
                        
                        # –í–µ—Ä—Å—ñ—è –∑–∞–∑–≤–∏—á–∞–π –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –≤ –¥—Ä—É–≥—ñ–π –∞–±–æ —Ç—Ä–µ—Ç—ñ–π –∫–æ–º—ñ—Ä—Ü—ñ
                        for i, cell in enumerate(all_cells[1:], start=1):  # –ü–æ—á–∏–Ω–∞—î–º–æ –∑ –¥—Ä—É–≥–æ—ó –∫–æ–º—ñ—Ä–∫–∏
                            cell_text = cell.get_text(strip=True)
                            
                            # –®—É–∫–∞—î–º–æ –Ω–æ–º–µ—Ä –≤–µ—Ä—Å—ñ—ó –≤ —Ç–µ–∫—Å—Ç—ñ –∫–æ–º—ñ—Ä–∫–∏
                            version = self.extract_version_from_text(cell_text)
                            if version:
                                print(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ –≤–µ—Ä—Å—ñ—é –¥–ª—è {model_name}: {version}")
                                return version
                        
                        # –Ø–∫—â–æ –≤ –∫–æ–º—ñ—Ä–∫–∞—Ö –Ω–µ –∑–Ω–∞–π—à–ª–∏ —á—ñ—Ç–∫–æ—ó –≤–µ—Ä—Å—ñ—ó, –¥–∏–≤–∏–º–æ—Å—å –ø–æ—Å–∏–ª–∞–Ω–Ω—è
                        for cell in all_cells[1:]:
                            links = cell.find_all('a')
                            for link in links:
                                link_text = link.get_text(strip=True)
                                version = self.extract_version_from_text(link_text)
                                if version:
                                    print(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ –≤–µ—Ä—Å—ñ—é –¥–ª—è {model_name} –≤ –ø–æ—Å–∏–ª–∞–Ω–Ω—ñ: {version}")
                                    return version
            
            # –Ø–∫—â–æ –Ω–µ –∑–Ω–∞–π—à–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É –º–æ–¥–µ–ª—å, –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ None
            print(f"‚ùå –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ")
            return None
            
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥—É Grandstream: {e}")
            return None

    def extract_version_from_text(self, text):
        """–í–∏—Ç—è–≥–Ω—É—Ç–∏ –Ω–æ–º–µ—Ä –≤–µ—Ä—Å—ñ—ó –∑ —Ç–µ–∫—Å—Ç—É"""
        patterns = [
                # 1. –í–µ—Ä—Å—ñ—ó –∑ –ø—Ä–µ—Ñ—ñ–∫—Å–æ–º (v, version, –≤–µ—Ä—Å—ñ—è)
                r'(?:v|version|–≤–µ—Ä—Å—ñ—è|–≤—ñ—Ä—Å—ñ—è|release|—Ä–µ–ª—ñ–∑|build|–±—ñ–ª–¥)\s*[:=]?\s*v?(\d+(?:\.\d+)+)',
                
                # 2. –í–µ—Ä—Å—ñ—ó –∑ 2-6 —á–∞—Å—Ç–∏–Ω–∞–º–∏ (1.0, 1.0.7, 1.0.7.79, 1.0.7.79.1)
                r'\b(\d+(?:\.\d+){1,5})\b',
                
                # 3. –í–µ—Ä—Å—ñ—ó –∑ –¥–∞—Ç–∞–º–∏ (2024.01.15.1)
                r'\b(\d{4}(?:\.\d+){1,3})\b',
                
                # 4. –í–µ—Ä—Å—ñ—ó –≤ –¥—É–∂–∫–∞—Ö/–∫–≤–∞–¥—Ä–∞—Ç–Ω–∏—Ö –¥—É–∂–∫–∞—Ö
                r'[\[(]v?(\d+(?:\.\d+)+)[])]',
                
                # 5. –í–µ—Ä—Å—ñ—ó –∑ –±—É–∫–≤–∞–º–∏ (1.0.7a, 2.0-beta, 3.1.4-rc1)
                r'\b(\d+(?:\.\d+)+[a-zA-Z]*(?:-\w+)?)\b',
                
                # 6. –í–µ—Ä—Å—ñ—ó –∑ —Ä–æ–∑–¥—ñ–ª—å–Ω–∏–∫–∞–º–∏ _ —ñ -
                r'\b(\d+(?:[_.-]\d+)+)\b',
                
                # 7. –ü—Ä–æ—Å—Ç–æ —á–∏—Å–ª–∞ –±—ñ–ª—å—à–µ 1000 (–º–æ–∂–µ –±—É—Ç–∏ –≤–µ—Ä—Å—ñ—î—é)
                r'\b(20\d{2}|\d{4,})\b',  # 2024, 12345
            ]
        

        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1)
        
        return None
    
    def check_specific_sites(self, url, name):
        """–°–ø–µ—Ü—ñ–∞–ª—å–Ω—ñ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω–∏—Ö —Å–∞–π—Ç—ñ–≤"""
        domain = urlparse(url).netloc.lower()
        
        # Grandstream
        if 'grandstream.com' in domain:
            # –¢—É—Ç –º–∏ –Ω–µ –º–æ–∂–µ–º–æ –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –º–æ–¥–µ–ª—å –∑ URL, —Ç–æ–º—É –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ None
            # –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞ –º–æ–¥–µ–ª—å –±—É–¥–µ –≤–∏–∑–Ω–∞—á–∞—Ç–∏—Å—è –≤ –æ—Å–Ω–æ–≤–Ω—ñ–π –ø—Ä–æ–≥—Ä–∞–º—ñ
            return None

        # –î–ª—è GitHub
        if 'github.com' in url:
            try:
                # –§–æ—Ä–º—É—î–º–æ API URL –¥–ª—è GitHub
                repo_path = url.replace('https://github.com/', '')
                api_url = f"https://api.github.com/repos/{repo_path}/releases/latest"
                response = self.session.get(api_url, timeout=10)
                if response.status_code == 200:
                    data = response.json()
                    return data.get('tag_name', '').replace('v', '')
            except:
                pass
        
        # –î–ª—è Docker Hub
        elif 'hub.docker.com' in url:
            try:
                response = self.session.get(url, timeout=10)
                soup = BeautifulSoup(response.text, 'html.parser')
                # –ü–æ—à—É–∫ –≤–µ—Ä—Å—ñ—ó –Ω–∞ Docker Hub
                tag_elements = soup.select('.TagList__tag-name')
                if tag_elements:
                    return tag_elements[0].text.strip()
            except:
                pass
        
        return None

# –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –ø–∞—Ä—Å–µ—Ä–∞
if __name__ == "__main__":
    parser = VersionParser()
    
    # –¢–µ—Å—Ç—É—î–º–æ –Ω–∞ Python —Å–∞–π—Ç—ñ
    version = parser.get_version_from_website(
        "https://www.python.org/downloads/",
        ".download-for-current-os .download-number"
    )
    print(f"–í–µ—Ä—Å—ñ—è Python: {version}")
    
    # –¢–µ—Å—Ç—É—î–º–æ –±–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞
    version = parser.get_version_from_website("https://www.videolan.org/vlc/")
    print(f"–í–µ—Ä—Å—ñ—è VLC: {version}")